{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T19:36:14.263195Z",
     "iopub.status.busy": "2024-12-12T19:36:14.262888Z",
     "iopub.status.idle": "2024-12-12T19:36:17.940149Z",
     "shell.execute_reply": "2024-12-12T19:36:17.939267Z",
     "shell.execute_reply.started": "2024-12-12T19:36:14.263166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification, pipeline\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Load only english texts\n",
    "dataset = load_dataset(\"ai4privacy/pii-masking-400k\")\n",
    "english_dataset = dataset.filter(lambda x: x['language'] == 'en')\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T19:36:17.941719Z",
     "iopub.status.busy": "2024-12-12T19:36:17.941385Z",
     "iopub.status.idle": "2024-12-12T19:36:17.947948Z",
     "shell.execute_reply": "2024-12-12T19:36:17.947152Z",
     "shell.execute_reply.started": "2024-12-12T19:36:17.941692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First lines of train:\n",
      "{'source_text': ['<p>My child faozzsd379223 (DOB: May/58) will undergo treatment with Dr. faozzsd379223, office at Hill Road. Our ZIP code is 28170-6392. Consult policy M.UE.227995. Contact number: 0070.606.322.6244. Handle transactions with 6225427220412963. Queries? Email: faozzsd379223@outlook.com.</p>', 'Guardians:*BF6* and *BF6* grant permission for their child *BF6*, born on *1960-08-01T00:00:00*, to participate in the Early Intervention Program at *Bicester Bucknell* University. The programme leader, Dr. *BF6* can be contacted at *52siddharta@aol.com* or *536373370485280*.', 'We, *bahara.cathers19* and *bahara.cathers19* reside at *358*, *Totnes*, *United Kingdom*, consent for our child *bahara.cathers19*, born on *August/72*, to participate. Please reach us at *18C@gmail.com* or *888517851168245*.'], 'locale': ['US', 'GB', 'GB'], 'language': ['en', 'en', 'en'], 'split': ['train', 'train', 'train'], 'privacy_mask': [[{'label': 'USERNAME', 'start': 12, 'end': 25, 'value': 'faozzsd379223', 'label_index': 2}, {'label': 'DATEOFBIRTH', 'start': 32, 'end': 38, 'value': 'May/58', 'label_index': 1}, {'label': 'USERNAME', 'start': 72, 'end': 85, 'value': 'faozzsd379223', 'label_index': 1}, {'label': 'STREET', 'start': 97, 'end': 106, 'value': 'Hill Road', 'label_index': 1}, {'label': 'ZIPCODE', 'start': 124, 'end': 134, 'value': '28170-6392', 'label_index': 1}, {'label': 'TELEPHONENUM', 'start': 180, 'end': 197, 'value': '0070.606.322.6244', 'label_index': 1}, {'label': 'CREDITCARDNUMBER', 'start': 224, 'end': 240, 'value': '6225427220412963', 'label_index': 1}, {'label': 'EMAIL', 'start': 258, 'end': 283, 'value': 'faozzsd379223@outlook.com', 'label_index': 1}], [{'label': 'USERNAME', 'start': 11, 'end': 14, 'value': 'BF6', 'label_index': 4}, {'label': 'USERNAME', 'start': 21, 'end': 24, 'value': 'BF6', 'label_index': 3}, {'label': 'USERNAME', 'start': 60, 'end': 63, 'value': 'BF6', 'label_index': 2}, {'label': 'DATEOFBIRTH', 'start': 75, 'end': 94, 'value': '1960-08-01T00:00:00', 'label_index': 1}, {'label': 'CITY', 'start': 150, 'end': 167, 'value': 'Bicester Bucknell', 'label_index': 1}, {'label': 'USERNAME', 'start': 208, 'end': 211, 'value': 'BF6', 'label_index': 1}, {'label': 'EMAIL', 'start': 234, 'end': 253, 'value': '52siddharta@aol.com', 'label_index': 1}], [{'label': 'USERNAME', 'start': 5, 'end': 21, 'value': 'bahara.cathers19', 'label_index': 3}, {'label': 'USERNAME', 'start': 28, 'end': 44, 'value': 'bahara.cathers19', 'label_index': 2}, {'label': 'BUILDINGNUM', 'start': 57, 'end': 60, 'value': '358', 'label_index': 1}, {'label': 'CITY', 'start': 64, 'end': 70, 'value': 'Totnes', 'label_index': 1}, {'label': 'USERNAME', 'start': 114, 'end': 130, 'value': 'bahara.cathers19', 'label_index': 1}, {'label': 'DATEOFBIRTH', 'start': 142, 'end': 151, 'value': 'August/72', 'label_index': 1}, {'label': 'EMAIL', 'start': 190, 'end': 203, 'value': '18C@gmail.com', 'label_index': 1}]], 'uid': [302521, 120409, 120411], 'masked_text': ['<p>My child [USERNAME_2] (DOB: [DATEOFBIRTH_1]) will undergo treatment with Dr. [USERNAME_1], office at [STREET_1]. Our ZIP code is [ZIPCODE_1]. Consult policy M.UE.227995. Contact number: [TELEPHONENUM_1]. Handle transactions with [CREDITCARDNUMBER_1]. Queries? Email: [EMAIL_1].</p>', 'Guardians:*[USERNAME_4]* and *[USERNAME_3]* grant permission for their child *[USERNAME_2]*, born on *[DATEOFBIRTH_1]*, to participate in the Early Intervention Program at *[CITY_1]* University. The programme leader, Dr. *[USERNAME_1]* can be contacted at *[EMAIL_1]* or *536373370485280*.', 'We, *[USERNAME_3]* and *[USERNAME_2]* reside at *[BUILDINGNUM_1]*, *[CITY_1]*, *United Kingdom*, consent for our child *[USERNAME_1]*, born on *[DATEOFBIRTH_1]*, to participate. Please reach us at *[EMAIL_1]* or *888517851168245*.'], 'mbert_tokens': [['<', 'p', '>', 'My', 'child', 'fa', '##oz', '##zs', '##d', '##3', '##7', '##9', '##22', '##3', '(', 'DO', '##B', ':', 'May', '/', '58', ')', 'will', 'under', '##go', 'treatment', 'with', 'Dr', '.', 'fa', '##oz', '##zs', '##d', '##3', '##7', '##9', '##22', '##3', ',', 'office', 'at', 'Hill', 'Road', '.', 'Our', 'Z', '##IP', 'code', 'is', '281', '##70', '-', '639', '##2', '.', 'Con', '##sul', '##t', 'policy', 'M', '.', 'UE', '.', '227', '##99', '##5', '.', 'Contact', 'number', ':', '007', '##0', '.', '606', '.', '322', '.', '624', '##4', '.', 'Hand', '##le', 'transaction', '##s', 'with', '622', '##5', '##42', '##7', '##22', '##04', '##12', '##9', '##6', '##3', '.', 'Que', '##ries', '?', 'Em', '##ail', ':', 'fa', '##oz', '##zs', '##d', '##3', '##7', '##9', '##22', '##3', '@', 'out', '##lo', '##ok', '.', 'com', '.', '<', '/', 'p', '>'], ['Guardian', '##s', ':', '*', 'BF', '##6', '*', 'and', '*', 'BF', '##6', '*', 'grant', 'permission', 'for', 'their', 'child', '*', 'BF', '##6', '*', ',', 'born', 'on', '*', '1960', '-', '08', '-', '01', '##T', '##00', ':', '00', ':', '00', '*', ',', 'to', 'participate', 'in', 'the', 'Early', 'Inter', '##vention', 'Program', 'at', '*', 'Bi', '##ces', '##ter', 'Buck', '##nell', '*', 'University', '.', 'The', 'programme', 'leader', ',', 'Dr', '.', '*', 'BF', '##6', '*', 'can', 'be', 'contacte', '##d', 'at', '*', '52', '##sid', '##dha', '##rta', '@', 'ao', '##l', '.', 'com', '*', 'or', '*', '536', '##3', '##7', '##33', '##70', '##48', '##52', '##80', '*', '.'], ['We', ',', '*', 'ba', '##hara', '.', 'cat', '##hers', '##19', '*', 'and', '*', 'ba', '##hara', '.', 'cat', '##hers', '##19', '*', 'reside', 'at', '*', '358', '*', ',', '*', 'Tot', '##nes', '*', ',', '*', 'United', 'Kingdom', '*', ',', 'consent', 'for', 'our', 'child', '*', 'ba', '##hara', '.', 'cat', '##hers', '##19', '*', ',', 'born', 'on', '*', 'August', '/', '72', '*', ',', 'to', 'participate', '.', 'Please', 'reach', 'us', 'at', '*', '18', '##C', '@', 'g', '##mail', '.', 'com', '*', 'or', '*', '888', '##51', '##7', '##85', '##11', '##6', '##82', '##45', '*', '.']], 'mbert_token_classes': [['O', 'O', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'O', 'B-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'B-STREET', 'I-STREET', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ZIPCODE', 'I-ZIPCODE', 'I-ZIPCODE', 'I-ZIPCODE', 'I-ZIPCODE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'I-TELEPHONENUM', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'I-CREDITCARDNUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'O', 'O', 'B-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'I-CITY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'O', 'B-BUILDINGNUM', 'O', 'O', 'O', 'B-CITY', 'I-CITY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'O', 'O', 'B-DATEOFBIRTH', 'I-DATEOFBIRTH', 'I-DATEOFBIRTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'I-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst lines of train:\")\n",
    "print(english_dataset['train'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T19:36:17.949911Z",
     "iopub.status.busy": "2024-12-12T19:36:17.949642Z",
     "iopub.status.idle": "2024-12-12T19:36:17.973944Z",
     "shell.execute_reply": "2024-12-12T19:36:17.973094Z",
     "shell.execute_reply.started": "2024-12-12T19:36:17.949887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         source_text locale language  split  \\\n",
      "0  <p>My child faozzsd379223 (DOB: May/58) will u...     US       en  train   \n",
      "1  Guardians:*BF6* and *BF6* grant permission for...     GB       en  train   \n",
      "2  We, *bahara.cathers19* and *bahara.cathers19* ...     GB       en  train   \n",
      "3  Student: Blagojka van der Boog\\nDOB: 8th Janua...     US       en  train   \n",
      "4  Child: Anna-Louise Dolderer\\nDate of Birth: 05...     US       en  train   \n",
      "\n",
      "                                        privacy_mask     uid  \\\n",
      "0  [{'label': 'USERNAME', 'start': 12, 'end': 25,...  302521   \n",
      "1  [{'label': 'USERNAME', 'start': 11, 'end': 14,...  120409   \n",
      "2  [{'label': 'USERNAME', 'start': 5, 'end': 21, ...  120411   \n",
      "3  [{'label': 'GIVENNAME', 'start': 9, 'end': 17,...  128429   \n",
      "4  [{'label': 'GIVENNAME', 'start': 7, 'end': 18,...  128431   \n",
      "\n",
      "                                         masked_text  \\\n",
      "0  <p>My child [USERNAME_2] (DOB: [DATEOFBIRTH_1]...   \n",
      "1  Guardians:*[USERNAME_4]* and *[USERNAME_3]* gr...   \n",
      "2  We, *[USERNAME_3]* and *[USERNAME_2]* reside a...   \n",
      "3  Student: [GIVENNAME_2] [SURNAME_2]\\nDOB: [DATE...   \n",
      "4  Child: [GIVENNAME_2] [SURNAME_2]\\nDate of Birt...   \n",
      "\n",
      "                                        mbert_tokens  \\\n",
      "0  [<, p, >, My, child, fa, ##oz, ##zs, ##d, ##3,...   \n",
      "1  [Guardian, ##s, :, *, BF, ##6, *, and, *, BF, ...   \n",
      "2  [We, ,, *, ba, ##hara, ., cat, ##hers, ##19, *...   \n",
      "3  [Student, :, B, ##lag, ##oj, ##ka, van, der, B...   \n",
      "4  [Child, :, Anna, -, Louise, Dol, ##dere, ##r, ...   \n",
      "\n",
      "                                 mbert_token_classes  \n",
      "0  [O, O, O, O, O, B-USERNAME, I-USERNAME, I-USER...  \n",
      "1  [O, O, O, O, B-USERNAME, I-USERNAME, O, O, O, ...  \n",
      "2  [O, O, O, B-USERNAME, I-USERNAME, I-USERNAME, ...  \n",
      "3  [O, O, B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, ...  \n",
      "4  [O, O, B-GIVENNAME, I-GIVENNAME, I-GIVENNAME, ...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(english_dataset['train'][:5])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T19:36:17.974956Z",
     "iopub.status.busy": "2024-12-12T19:36:17.974725Z",
     "iopub.status.idle": "2024-12-12T19:36:17.985879Z",
     "shell.execute_reply": "2024-12-12T19:36:17.984964Z",
     "shell.execute_reply.started": "2024-12-12T19:36:17.974934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: source_text, Type: <class 'str'>\n",
      "Column: locale, Type: <class 'str'>\n",
      "Column: language, Type: <class 'str'>\n",
      "Column: split, Type: <class 'str'>\n",
      "Column: privacy_mask, Type: <class 'list'>\n",
      "Column: uid, Type: <class 'int'>\n",
      "Column: masked_text, Type: <class 'str'>\n",
      "Column: mbert_tokens, Type: <class 'list'>\n",
      "Column: mbert_token_classes, Type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for column in english_dataset['train'].column_names:\n",
    "    first_value = english_dataset['train'][0][column]\n",
    "    print(f\"Column: {column}, Type: {type(first_value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T19:36:17.987321Z",
     "iopub.status.busy": "2024-12-12T19:36:17.987001Z",
     "iopub.status.idle": "2024-12-12T19:36:18.005368Z",
     "shell.execute_reply": "2024-12-12T19:36:18.004587Z",
     "shell.execute_reply.started": "2024-12-12T19:36:17.987284Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_privacy_masks(privacy_masks):\n",
    "    \"\"\"\n",
    "    Parse privacy masks from various input formats into a consistent list\n",
    "    \"\"\"\n",
    "    # If input is a string, try parsing as JSON or using eval\n",
    "    if isinstance(privacy_masks, str):\n",
    "        try:\n",
    "            # parse as JSON, replacing single quotes with double quotes\n",
    "            return json.loads(privacy_masks.replace(\"'\", '\"'))\n",
    "        except:\n",
    "            try:\n",
    "                return eval(privacy_masks)\n",
    "            except:\n",
    "                # Both parsing methods fail\n",
    "                return []\n",
    "    \n",
    "    # If input is already a list, no parsing needed\n",
    "    elif isinstance(privacy_masks, list):\n",
    "        return privacy_masks\n",
    "    return []\n",
    "\n",
    "def extract_labels(privacy_masks):\n",
    "    \"\"\"\n",
    "    Extract unique labels from privacy masks\n",
    "    \"\"\"\n",
    "    masks = parse_privacy_masks(privacy_masks)\n",
    "    return set(mask['label'] for mask in masks)\n",
    "\n",
    "def prepare_model_and_tokenizer(dataset):\n",
    "    \"\"\"\n",
    "    Prepare the model and tokenizer for named entity recognition\n",
    "    \"\"\"\n",
    "    # Collect all unique labels from the training dataset\n",
    "    all_labels = set()\n",
    "    for privacy_mask in dataset['train']['privacy_mask']:\n",
    "        all_labels.update(extract_labels(privacy_mask))\n",
    "    \n",
    "    # Create label list with BIO (Begin, Inside, Outside)\n",
    "    # 'O' represents tokens not part of any named entity\n",
    "    # 'B-' prefix marks the beginning of an entity\n",
    "    # 'I-' prefix marks inside/continuation of an entity\n",
    "    label_list = ['O'] + [f'B-{label}' for label in all_labels] + [f'I-{label}' for label in all_labels]\n",
    "    \n",
    "    # Create bidirectional label-id mappings\n",
    "    label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    \n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_checkpoint = \"bert-base-cased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(label_list),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    return tokenizer, model, label_list, label2id\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer, label2id):\n",
    "    \"\"\"\n",
    "    Tokenize input text and align privacy mask labels with tokenized inputs\n",
    "    \"\"\"\n",
    "    # Tokenize input text\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['source_text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=False,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    # Process batch\n",
    "    for i, source_text in enumerate(examples['source_text']):\n",
    "        # Get token offsets and word ids for the current example\n",
    "        offsets = tokenized_inputs['offset_mapping'][i]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        # Initialize labels with 'O' (Outside) (BIO)\n",
    "        label = np.full(len(offsets), label2id['O'], dtype=int)\n",
    "        \n",
    "        # Process privacy masks\n",
    "        privacy_masks = parse_privacy_masks(examples['privacy_mask'][i])\n",
    "        \n",
    "        # Assign labels based on token offsets and privacy masks\n",
    "        for mask in privacy_masks:\n",
    "            start, end, label_type = mask['start'], mask['end'], mask['label']\n",
    "            \n",
    "            # Check each token's alignment with privacy mask\n",
    "            for j, (token_start, token_end) in enumerate(offsets):\n",
    "                if word_ids[j] is None:\n",
    "                    continue\n",
    "                \n",
    "                # Check if token is within the privacy mask span\n",
    "                # First token gets 'B-' (Begin) (BIO) \n",
    "                # the following tokens get 'I-' (Inside) (BIO)\n",
    "                if token_start >= start and token_end <= end:\n",
    "                    if token_start == start:\n",
    "                        label[j] = label2id[f'B-{label_type}']\n",
    "                    else:\n",
    "                        label[j] = label2id[f'I-{label_type}']\n",
    "        \n",
    "        labels.append(label)\n",
    "    \n",
    "    # Remove offset mapping and add labels to tokenized inputs\n",
    "    tokenized_inputs.pop(\"offset_mapping\")\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for model predictions.\n",
    "    \"\"\"\n",
    "    # Extract true labels and model predictions\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Create a mask to ignore special tokens\n",
    "    mask = labels != -100\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels[mask], preds[mask], average='weighted'\n",
    "    )\n",
    "    acc = accuracy_score(labels[mask], preds[mask])\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T19:36:18.006627Z",
     "iopub.status.busy": "2024-12-12T19:36:18.006352Z",
     "iopub.status.idle": "2024-12-12T21:38:31.590400Z",
     "shell.execute_reply": "2024-12-12T21:38:31.589651Z",
     "shell.execute_reply.started": "2024-12-12T19:36:18.006602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d1d4fe43b44bed9db9a5eea7711721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc13ba9c8e15472283fc65f0b3c89f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f37b3b781db42cc84f7ca5d1b7de509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ae1f7607c8438e8479c330d992b034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73087c822951441da6d7b44ac47326ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b68501464a4ad9822d05a25e98f381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/68275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6943e995b9a45cf96bc9872af88deeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17046 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25605' max='25605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25605/25605 2:01:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>0.995917</td>\n",
       "      <td>0.996130</td>\n",
       "      <td>0.995917</td>\n",
       "      <td>0.995983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.009314</td>\n",
       "      <td>0.996661</td>\n",
       "      <td>0.996664</td>\n",
       "      <td>0.996661</td>\n",
       "      <td>0.996648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.996965</td>\n",
       "      <td>0.996967</td>\n",
       "      <td>0.996965</td>\n",
       "      <td>0.996958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25605, training_loss=0.013246627515947544, metrics={'train_runtime': 7302.5266, 'train_samples_per_second': 28.049, 'train_steps_per_second': 3.506, 'total_flos': 3.15995291723052e+16, 'train_loss': 0.013246627515947544, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, model, label_list, label2id = prepare_model_and_tokenizer(english_dataset)\n",
    "\n",
    "tokenized_datasets = english_dataset.map(\n",
    "    lambda examples: tokenize_and_align_labels(examples, tokenizer, label2id),\n",
    "    batched=True,\n",
    "    remove_columns=english_dataset['train'].column_names\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"  # Desactivate WandB\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T21:38:45.755917Z",
     "iopub.status.busy": "2024-12-12T21:38:45.755185Z",
     "iopub.status.idle": "2024-12-12T21:38:47.200671Z",
     "shell.execute_reply": "2024-12-12T21:38:47.199770Z",
     "shell.execute_reply.started": "2024-12-12T21:38:45.755882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./saved_model\"\n",
    "\n",
    "def save_trained_model(model_path):\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "save_trained_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T21:38:54.221599Z",
     "iopub.status.busy": "2024-12-12T21:38:54.220756Z",
     "iopub.status.idle": "2024-12-12T21:38:54.225875Z",
     "shell.execute_reply": "2024-12-12T21:38:54.225058Z",
     "shell.execute_reply.started": "2024-12-12T21:38:54.221527Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_trained_model(model_path=\"./saved_model\"):\n",
    "    \"\"\"\n",
    "    Load the trained model from the result directory\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    print(\"Model reloaded\")\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T21:38:54.230389Z",
     "iopub.status.busy": "2024-12-12T21:38:54.230073Z",
     "iopub.status.idle": "2024-12-12T21:38:55.006821Z",
     "shell.execute_reply": "2024-12-12T21:38:55.005920Z",
     "shell.execute_reply.started": "2024-12-12T21:38:54.230355Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded\n",
      "\n",
      "Original text:\n",
      " My name is John Doe and my email is john.doe@example.com. My phone number is 123-456-7890.\n",
      "\n",
      "Masked text:\n",
      " My name is John Doe and my email is [EMAIL]. My phone number is [TELEPHONENUM].\n",
      "\n",
      "PII detected:\n",
      "- john. doe @ example. com: EMAIL (confidence: 1.00)\n",
      "- 123 - 456 - 7890: TELEPHONENUM (confidence: 1.00)\n",
      "\n",
      "Original text:\n",
      " I live at 123 Main Street, Anytown, USA 12345. My credit card is 4111-1111-1111-1111.\n",
      "\n",
      "Masked text:\n",
      " I live at [BUILDINGNUM] [STREET] Street, [CITY], USA [ZIPCODE]. My credit card is [TELEPHONENUM].\n",
      "\n",
      "PII detected:\n",
      "- 123: BUILDINGNUM (confidence: 1.00)\n",
      "- Main: STREET (confidence: 0.51)\n",
      "- Anytown: CITY (confidence: 1.00)\n",
      "- 12345: ZIPCODE (confidence: 0.88)\n",
      "- 4111 - 1111 - 1111 - 1111: TELEPHONENUM (confidence: 0.86)\n",
      "\n",
      "Original text:\n",
      " Contact Dr. Smith at his office: 0070.606.322.6244 or email: doctor@hospital.org\n",
      "\n",
      "Masked text:\n",
      " Contact Dr. Smith at his office: [TELEPHONENUM] or email: doctor@hospital.org\n",
      "\n",
      "PII detected:\n",
      "- 0070. 606. 322. 6244: TELEPHONENUM (confidence: 1.00)\n",
      "\n",
      "Original text:\n",
      " John Doe lives in Paris, and his phone number is 123-456-7890.\n",
      "\n",
      "Masked text:\n",
      " John Doe lives in [CITY], and his phone number is [TELEPHONENUM].\n",
      "\n",
      "PII detected:\n",
      "- Paris: CITY (confidence: 1.00)\n",
      "- 123 - 456 - 7890: TELEPHONENUM (confidence: 1.00)\n",
      "\n",
      "Original text:\n",
      " Alice lives in New York. Her email is alice@example.com.\n",
      "\n",
      "Masked text:\n",
      " Alice lives in [CITY]. Her email is [EMAIL].\n",
      "\n",
      "PII detected:\n",
      "- New York: CITY (confidence: 0.99)\n",
      "- alice @ example. com: EMAIL (confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "def create_pipeline(tokenizer, model):\n",
    "    \"\"\"\n",
    "    Create a named entity recognition (NER) pipeline\n",
    "    \"\"\"\n",
    "    return pipeline(\n",
    "        \"ner\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        aggregation_strategy=\"simple\"\n",
    "    )\n",
    "\n",
    "def test_pii_detection(ner_pipeline, masked_text):\n",
    "    \"\"\"\n",
    "    Detect and mask (PII) in a text\n",
    "    \"\"\"\n",
    "    # Run the NER pipeline\n",
    "    pipeline = ner_pipeline(masked_text)\n",
    "    \n",
    "    # Sort the results by the start position to avoid offset issues\n",
    "    sorted_results = sorted(pipeline, key=lambda x: x['start'], reverse=True)\n",
    "    \n",
    "    # Mask each detected entity\n",
    "    for entity in sorted_results:\n",
    "        start, end = entity['start'], entity['end']\n",
    "        label = entity['entity_group']\n",
    "        \n",
    "        masked_text = masked_text[:start] + f\"[{label}]\" + masked_text[end:]\n",
    "    \n",
    "    return masked_text, pipeline\n",
    "\n",
    "def main():\n",
    "    test_texts = [\n",
    "        \"My name is John Doe and my email is john.doe@example.com. My phone number is 123-456-7890.\",\n",
    "        \"I live at 123 Main Street, Anytown, USA 12345. My credit card is 4111-1111-1111-1111.\",\n",
    "        \"Contact Dr. Smith at his office: 0070.606.322.6244 or email: doctor@hospital.org\",\n",
    "        \"John Doe lives in Paris, and his phone number is 123-456-7890.\",\n",
    "        \"Alice lives in New York. Her email is alice@example.com.\"\n",
    "    ]\n",
    "    \n",
    "    # Load model and create pipeline\n",
    "    tokenizer, model = load_trained_model()\n",
    "    ner_pipeline = create_pipeline(tokenizer, model)\n",
    "    \n",
    "    # Tester chaque texte\n",
    "    for text in test_texts:\n",
    "        print(\"\\nOriginal text:\\n\", text)\n",
    "        \n",
    "        masked_text, entities = test_pii_detection(ner_pipeline, text)\n",
    "        \n",
    "        print(\"\\nMasked text:\\n\", masked_text)\n",
    "        \n",
    "        print(\"\\nPII detected:\")\n",
    "        for entity in entities:\n",
    "            print(f\"- {entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.2f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model In Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T21:38:55.009835Z",
     "iopub.status.busy": "2024-12-12T21:38:55.009450Z",
     "iopub.status.idle": "2024-12-12T21:38:56.046884Z",
     "shell.execute_reply": "2024-12-12T21:38:56.045895Z",
     "shell.execute_reply.started": "2024-12-12T21:38:55.009797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json        special_tokens_map.json  tokenizer_config.json\n",
      "model.safetensors  tokenizer.json           vocab.txt\n"
     ]
    }
   ],
   "source": [
    "ls \"./saved_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T21:38:56.048470Z",
     "iopub.status.busy": "2024-12-12T21:38:56.048192Z",
     "iopub.status.idle": "2024-12-12T21:39:18.962066Z",
     "shell.execute_reply": "2024-12-12T21:39:18.961117Z",
     "shell.execute_reply.started": "2024-12-12T21:38:56.048444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: saved_model/ (stored 0%)\n",
      "  adding: saved_model/config.json (deflated 64%)\n",
      "  adding: saved_model/vocab.txt (deflated 49%)\n",
      "  adding: saved_model/special_tokens_map.json (deflated 42%)\n",
      "  adding: saved_model/tokenizer_config.json (deflated 76%)\n",
      "  adding: saved_model/model.safetensors (deflated 7%)\n",
      "  adding: saved_model/tokenizer.json (deflated 70%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r saved_model.zip saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T21:39:18.963800Z",
     "iopub.status.busy": "2024-12-12T21:39:18.963483Z",
     "iopub.status.idle": "2024-12-12T21:39:18.970464Z",
     "shell.execute_reply": "2024-12-12T21:39:18.969684Z",
     "shell.execute_reply.started": "2024-12-12T21:39:18.963773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='saved_model.zip' target='_blank'>saved_model.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/saved_model.zip"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(\"saved_model.zip\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
